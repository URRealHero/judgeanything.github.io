<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Judge Anything: MLLM as a Judge Across Any Modality">
  <meta name="keywords" content="MLLM-as-a-Judge, Multimodal Understanding, Multimodal Generation, Benchmark, Dataset, Any-to-Any Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Judge Anything: MLLM as a Judge Across Any Modality</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://urrealhero.github.io/MyPersonalWeb/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Judge Anything: MLLM as a Judge Across Any Modality</h1>
          <div class="is-size-5 publication-authors">
            <!-- Co-first authors -->
            <span class="author-block">
              <a href="https://urrealhero.github.io/MyPersonalWeb/">Shu Pu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a>Yaochen Wang</a><sup>1*</sup>,</span>
            
            <!-- Project leader -->
            <span class="author-block">
              <a href="https://dongping-chen.github.io/">Dongping Chen</a><sup>1</sup>,
            </span>
            
            <!-- Co-authors (4th-10th) -->
            <span class="author-block">
              <a>Yuhang Chen</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a>Guohao Wang</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a>Qi Qin</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a>Zhongyi Zhang</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a>Zhiyuan Zhang</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a>Zetong Zhou</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a>Shuang Gong</a><sup>1†</sup>,</span>
            
            <!-- Regular author -->
            <span class="author-block">
              <a>Yi Gui</a><sup>1</sup>,</span>
            
            <!-- Corresponding author -->
            <span class="author-block">
              <a href="http://wanyao.me/">Yao Wan</a><sup>1§</sup>,
            </span>
            
            <!-- External collaborator -->
            <span class="author-block">
              <a href="https://www.cs.uic.edu/~psyu/">Philip S. Yu</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science and Technology,</span>
            <span class="author-block"><sup>2</sup>University of Illinois Chicago</span><br>
            
            <!-- Annotation symbols -->
            <span class="author-block">
              * Co-first authors
              § Corresponding author<br>
              † Equal contribution
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/URRealHero/JudgeAnything"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/pudashi/JudgeAnything"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 替换视频为图片 -->
      <div class="has-text-centered">
        <img id="teaser" 
             src="./static/images/Main.jpg"  
             alt="Judge Anything Pipeline"
             style="max-height: 600px; width: auto;">  <!-- 控制图片尺寸 -->
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Construction Pipeline</span>  The construction of <em>TaskAnything</em> and <em>JudgeAnything</em> follows a systematic four-step approach. First,
        we compile open-ended any-to-any instructions from existing benchmarks and datasets, followed by rigorous
        human annotation to ensure sample diversity and quality in <em>TaskAnything</em>. Subsequently, we collect model
        responses and develop evaluation principles through an Human-MLLM collaborative approach, creating detailed
        assessment checklists for each sample. Finally, we curate instruction-responses pairs to evaluate the effectiveness
        of MLLM-as-a-Judge in any-to-any generation tasks, benchmarking these automated assessments against expert
        human judgments.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (<b>e.g.</b>, images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. 
            To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks.
            Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, <em>TaskAnything</em> and <em>JudgeAnything</em>, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks.
            Specifically, <em>TaskAnything</em> evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks.
            Furthermore, <em>JudgeAnything</em> evaluates the judging capabilities of 5 advanced (<b>e.g.</b>, GPT-4o and Gemini-1.5-Pro) from the perspectives of <em>Pair Comparison</em> and <em>Score Evaluation</em>, providing a standardized testbed that incorporates human judgments and detailed rubrics.
            Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (<b>i.e.</b>, achieving an average of 64.1% in <em>Pair Comparison</em> setting and 69.58% in <em>Score Evaluation</em> setting), they encounter significant challenges with MMG tasks (<b>i.e.</b>, averaging only 50.7% in <em>Pair Comparison</em> setting and 47.2\% in <em>Score Evaluation</em> setting), exposing cross-modality biases and hallucination issues.
            To address this, we present <em>OmniArena</em>, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences.
            The source code and dataset are publicly available at: \url{https://mllm-judge.github.io}.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->



  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="has-text-centered mb-6">
      <h1 class="title is-2">TaskAnything</h1>
    </div>

    <div class="content">
      <p>
        We collect samples from previous well-constructed and data-balanced benchmarks, as shown in Table 5, 
        followed by manual selection to filter out similar and non-open-source samples. For MMU tasks (e.g., 
        Video-to-Text), we further incorporate human refinements to remove predefined constraints and ensure 
        natural, free-form outputs. For MMG tasks (e.g., text-to-video), we filter out NSFW content and low-quality 
        queries to ensure answerability.
      </p>
      
      <p>
        For emerging task categories like visual-to-audio, we employ a human-in-the-loop approach to curate 
        diverse queries sourced from carefully filtered video datasets. These datasets are collected from 
        publicly available video platforms, ensuring both relevance and diversity. Through this rigorous process, 
        we have successfully constructed a comprehensive open-ended any-to-any benchmark comprising 1,500 
        high-quality queries, with 100 representative samples per task category.
      </p>
    </div>
        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Benchmark Overview</h2>
            <div class="publication-image">
              <img src="static/images/TaskAnythingTable.jpg" 
                   alt="Task overview infographic"
                   style="width: 100%; max-width: 800px; border-radius: 8px;">
            </div>
          </div>
        </div>
        <!--/ Paper video. -->
  </div>
  
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- 新增基线模块 -->

    <div class="has-text-centered mb-6">
      <h1 class="title is-2">JudgeAnything</h1>
    </div>
    <div class="has-text-centered mb-5">
      <h2 class="title is-3">Settings</h2>
    </div>
    <div class="columns is-centered">
      <!-- Pair. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Pair Comparison</h2>
          <p>
            <em>Pair Comparison</em> setting require judging models to choose a better choice from two given models responses.
          </p>
        </div>
      </div>
      <!--/ Pair. -->

      <!-- Score. -->
      <div class="column">
        <h2 class="title is-4">Score Evaluation</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              <em>Score Evaluation</em> setting require judgine models to score an integar with detailed scoring rules.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Score. -->

    <!-- 新增基线模块 -->
    <div class="has-text-centered mb-5">
      <h2 class="title is-3">Baselines</h2>
    </div>

    <div class="columns is-flex is-flex-wrap-wrap is-flex-grow-1">
      <!-- Overall -->
      <div class="column is-one-third-desktop">
        <div class="box p-5 has-background-white-ter h-100">
          <h3 class="title is-4 has-text-centered">Overall</h3>
          <div class="content">
            <ul class="has-text-left">
              <li>Single-step direct evaluation</li>
              <li>Combined reasoning and judgment</li>
              <li>Baseline for efficiency comparison</li>
            </ul>
          </div>
        </div>
      </div>

      <!-- Rubric -->
      <div class="column is-one-third-desktop">
        <div class="box p-5 has-background-warning-light h-100">
          <h3 class="title is-4 has-text-centered">Rubric</h3>
          <div class="content">
            <ul class="has-text-left">
              <li>Multi-criteria evaluation framework</li>
              <li>Predefined assessment dimensions</li>
              <li>Structured scoring system</li>
            </ul>
          </div>
        </div>
      </div>

      <!-- Checklist -->
      <div class="column is-one-third-desktop">
        <div class="box p-5 has-background-info-light h-100">
          <h3 class="title is-4 has-text-centered">Checklist</h3>
          <div class="content">
            <ul class="has-text-left">
              <li>Human-curated verification items</li>
              <li>Multi-stage validation process</li>
              <li>Comprehensive coverage assurance</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="table-container mb-6">
      <div class="box">
      <h3 class="title is-4 mb-4">Model Performance</h3>
      <div class="scrollable-table">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
      <thead>
      <tr class="has-background-grey-light">
      <th rowspan="2" class="has-text-centered">Models</th>
      <th colspan="6" class="has-text-centered has-background-info-light">Multimodal Understanding</th>
      <th colspan="6" class="has-text-centered has-background-success-light">Multimodal Generation</th>
      </tr>
      <tr class="has-background-grey-lighter">
      <th colspan="2" class="has-text-centered">Pair Comparison</th>
      <th colspan="4" class="has-text-centered">Score Evaluation</th>
      <th colspan="2" class="has-text-centered">Pair Comparison</th>
      <th colspan="4" class="has-text-centered">Score Evaluation</th>
      </tr>
      <tr>
      <th></th>
      <th>w. Tie</th>
      <th>w.o. Tie</th>
      <th>Agreement</th>
      <th>Pearson</th>
      <th>Spearman</th>
      <th>MAE</th>
      <th>w. Tie</th>
      <th>w.o. Tie</th>
      <th>Agreement</th>
      <th>Pearson</th>
      <th>Spearman</th>
      <th>MAE</th>
      </tr>
      </thead>
      <tbody>
      
      <!-- Overall Section -->
      <tr><td colspan="13" class="has-text-centered has-background-grey-lighter"><strong>Overall</strong></td></tr>
      <tr>
      <td>GPT-4o</td>
      <td>59.88</td>
      <td>75.19</td>
      <td><strong>38.55</strong></td>
      <td>0.488</td>
      <td>0.449</td>
      <td><strong>0.895</strong></td>
      <td>55.54</td>
      <td>69.78</td>
      <td>34.02</td>
      <td>0.476</td>
      <td>0.467</td>
      <td><strong>1.113</strong></td>
      </tr>
      <tr>
      <td>Gemini-1.5-Pro</td>
      <td><strong>63.39</strong></td>
      <td><strong>80.48</strong></td>
      <td>37.60</td>
      <td><u>0.494</u></td>
      <td><u>0.467</u></td>
      <td>0.985</td>
      <td><strong>60.71</strong></td>
      <td><strong>74.43</strong></td>
      <td><u>36.43</u></td>
      <td>0.404</td>
      <td>0.413</td>
      <td>1.185</td>
      </tr>
      <tr>
      <td>LearnLM-1.5-Pro</td>
      <td><u>62.37</u></td>
      <td><u>79.60</u></td>
      <td>35.32</td>
      <td>0.473</td>
      <td>0.447</td>
      <td>1.039</td>
      <td>57.48</td>
      <td>70.72</td>
      <td>36.13</td>
      <td>0.377</td>
      <td>0.384</td>
      <td>1.247</td>
      </tr>
      <tr>
      <td>Gemini-2.0-Flash</td>
      <td>55.57</td>
      <td>73.21</td>
      <td>34.85</td>
      <td>0.437</td>
      <td>0.350</td>
      <td>1.054</td>
      <td>53.69</td>
      <td>67.42</td>
      <td><strong>36.78</strong></td>
      <td>0.460</td>
      <td><u>0.460</u></td>
      <td>1.372</td>
      </tr>
      <tr>
      <td>Gemini-2.0-Flash-Lite</td>
      <td>52.16</td>
      <td>68.75</td>
      <td>36.00</td>
      <td>0.454</td>
      <td>0.386</td>
      <td>1.036</td>
      <td>45.81</td>
      <td>59.67</td>
      <td>34.13</td>
      <td>0.458</td>
      <td>0.442</td>
      <td>1.347</td>
      </tr>
      <tr>
      <td>Evaluator-Fusion</td>
      <td>61.79</td>
      <td>78.57</td>
      <td><u>37.95</u></td>
      <td><strong>0.549</strong></td>
      <td><strong>0.513</strong></td>
      <td><u>0.902</u></td>
      <td><u>57.50</u></td>
      <td><u>73.09</u></td>
      <td>30.10</td>
      <td><strong>0.556</strong></td>
      <td><strong>0.555</strong></td>
      <td><u>1.120</u></td>
      </tr>
      
      <!-- Rubrics Section -->
      <tr><td colspan="13" class="has-text-centered has-background-grey-lighter"><strong>Rubrics</strong></td></tr>
      <tr>
      <td>GPT-4o</td>
      <td>65.33</td>
      <td>75.45</td>
      <td>43.78</td>
      <td><u>0.636</u></td>
      <td><u>0.620</u></td>
      <td><strong>0.845</strong></td>
      <td>32.32</td>
      <td>52.92</td>
      <td>31.85</td>
      <td>0.415</td>
      <td>0.421</td>
      <td>1.289</td>
      </tr>
      <tr>
      <td>Gemini-1.5-Pro</td>
      <td><strong>71.80</strong></td>
      <td><strong>81.69</strong></td>
      <td><u>44.75</u></td>
      <td>0.626</td>
      <td>0.608</td>
      <td><u>0.846</u></td>
      <td><strong>61.38</strong></td>
      <td><strong>73.05</strong></td>
      <td><strong>38.81</strong></td>
      <td><u>0.459</u></td>
      <td><u>0.464</u></td>
      <td><strong>1.122</strong></td>
      </tr>
      <tr>
      <td>LearnLM-1.5-Pro</td>
      <td><u>69.46</u></td>
      <td><u>79.07</u></td>
      <td><strong>45.13</strong></td>
      <td>0.618</td>
      <td>0.607</td>
      <td>0.869</td>
      <td><u>59.74</u></td>
      <td><u>70.08</u></td>
      <td><u>38.52</u></td>
      <td>0.436</td>
      <td>0.440</td>
      <td><u>1.161</u></td>
      </tr>
      <tr>
      <td>Gemini-2.0-Flash</td>
      <td>46.83</td>
      <td>67.45</td>
      <td>41.16</td>
      <td>0.555</td>
      <td>0.525</td>
      <td>1.035</td>
      <td>39.88</td>
      <td>60.53</td>
      <td>30.42</td>
      <td>0.377</td>
      <td>0.384</td>
      <td>1.589</td>
      </tr>
      <tr>
      <td>Gemini-2.0-Flash-Lite</td>
      <td>52.62</td>
      <td>68.16</td>
      <td>39.96</td>
      <td>0.562</td>
      <td>0.549</td>
      <td>1.063</td>
      <td>40.52</td>
      <td>57.85</td>
      <td>31.82</td>
      <td>0.433</td>
      <td>0.443</td>
      <td>1.467</td>
      </tr>
      <tr>
      <td>Evaluator-Fusion</td>
      <td>67.87</td>
      <td>78.81</td>
      <td>40.31</td>
      <td><strong>0.692</strong></td>
      <td><strong>0.687</strong></td>
      <td>0.904</td>
      <td>51.55</td>
      <td>66.68</td>
      <td>27.18</td>
      <td><strong>0.556</strong></td>
      <td><strong>0.569</strong></td>
      <td>1.242</td>
      </tr>
      
      <!-- Checklist Section -->
      <tr><td colspan="13" class="has-text-centered has-background-grey-lighter"><strong>Checklist</strong></td></tr>
      <tr>
      <td>GPT-4o</td>
      <td>62.35</td>
      <td>73.65</td>
      <td>46.83</td>
      <td>0.685</td>
      <td>0.663</td>
      <td>0.744</td>
      <td>30.22</td>
      <td>51.94</td>
      <td>32.93</td>
      <td>0.366</td>
      <td>0.364</td>
      <td>1.259</td>
      </tr>
      <tr>
      <td>Gemini-1.5-Pro</td>
      <td><strong>74.95</strong></td>
      <td><strong>85.19</strong></td>
      <td><strong>62.83</strong></td>
      <td><strong>0.815</strong></td>
      <td><strong>0.796</strong></td>
      <td><strong>0.489</strong></td>
      <td><strong>64.39</strong></td>
      <td><strong>75.96</strong></td>
      <td><strong>48.32</strong></td>
      <td><u>0.585</u></td>
      <td><u>0.587</u></td>
      <td><strong>0.883</strong></td>
      </tr>
      <tr>
      <td>LearnLM-1.5-Pro</td>
      <td><u>69.35</u></td>
      <td>78.98</td>
      <td><u>49.51</u></td>
      <td>0.717</td>
      <td>0.700</td>
      <td>0.726</td>
      <td><u>60.04</u></td>
      <td>70.45</td>
      <td><u>40.53</u></td>
      <td>0.478</td>
      <td>0.479</td>
      <td>1.086</td>
      </tr>
      <tr>
      <td>Gemini-2.0-Flash</td>
      <td>53.87</td>
      <td>70.16</td>
      <td>44.26</td>
      <td>0.605</td>
      <td>0.586</td>
      <td>0.892</td>
      <td>49.81</td>
      <td>67.73</td>
      <td>37.93</td>
      <td>0.483</td>
      <td>0.487</td>
      <td>1.222</td>
      </tr>
      <tr>
      <td>Gemini-2.0-Flash-Lite</td>
      <td>56.03</td>
      <td>68.28</td>
      <td>43.07</td>
      <td>0.597</td>
      <td>0.590</td>
      <td>0.901</td>
      <td>49.11</td>
      <td>65.47</td>
      <td>36.68</td>
      <td>0.439</td>
      <td>0.445</td>
      <td>1.175</td>
      </tr>
      <tr>
      <td>Evaluator-Fusion</td>
      <td>68.22</td>
      <td><u>79.28</u></td>
      <td>46.80</td>
      <td><u>0.756</u></td>
      <td><u>0.746</u></td>
      <td><u>0.718</u></td>
      <td>59.29</td>
      <td><u>72.42</u></td>
      <td>33.14</td>
      <td><strong>0.600</strong></td>
      <td><strong>0.606</strong></td>
      <td><u>0.992</u></td>
      </tr>
      
      </tbody>
      </table>
      </div>
      </div>
      </div>

      <div class="table-container">
        <div class="box">
        <h3 class="title is-4 mb-4">Task-wise Performance Breakdown</h3>
        <div class="scrollable-table">
        <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
        <tr class="has-background-grey-light">
        <th rowspan="2" class="has-text-centered">Models</th>
        <th rowspan="2" class="has-text-centered">Setting</th>
        <th rowspan="2" class="has-text-centered">Overall</th>
        <th colspan="5" class="has-text-centered has-background-info-light">Multimodal Understanding</th>
        <th colspan="10" class="has-text-centered has-background-success-light">Multimodal Generation</th>
        </tr>
        <tr class="has-background-grey-lighter">
        <th>T&rarr;T</th>
        <th>I&rarr;T</th>
        <th>V&rarr;T</th>
        <th>A&rarr;T</th>
        <th>V+A&rarr;T</th>
        <th>T&rarr;I</th>
        <th>T&rarr;V</th>
        <th>T&rarr;A</th>
        <th>I&rarr;I</th>
        <th>I&rarr;V</th>
        <th>I&rarr;A</th>
        <th>V&rarr;V</th>
        <th>V&rarr;A</th>
        <th>A&rarr;V</th>
        <th>A&rarr;A</th>
        </tr>
        </thead>
        <tbody>
        
        <!-- Pair Section -->
        <tr><td colspan="18" class="has-text-centered has-background-grey-lighter"><strong>Pair</strong></td></tr>
        <tr>
        <td rowspan="3">GPT-4o</td>
        <td>Overall</td>
        <td><strong>56.99</strong></td>
        <td><strong>52.50</strong></td>
        <td><strong>71.07</strong></td>
        <td>58.00</td>
        <td>65.50</td>
        <td>52.50</td>
        <td>52.00</td>
        <td><strong>54.00</strong></td>
        <td>31.00</td>
        <td><strong>53.77</strong></td>
        <td><strong>53.54</strong></td>
        <td><strong>67.50</strong></td>
        <td><strong>79.00</strong></td>
        <td><strong>43.50</strong></td>
        <td><strong>62.00</strong></td>
        <td><strong>59.18</strong></td>
        </tr>
        <tr>
        <td>Rubric</td>
        <td>43.33</td>
        <td>51.67</td>
        <td>66.50</td>
        <td><strong>64.67</strong></td>
        <td><strong>76.50</strong></td>
        <td><strong>67.33</strong></td>
        <td><strong>78.17</strong></td>
        <td>23.33</td>
        <td><strong>42.25</strong></td>
        <td>44.17</td>
        <td>31.75</td>
        <td>16.42</td>
        <td>2.33</td>
        <td>28.17</td>
        <td>47.92</td>
        <td>8.75</td>
        </tr>
        <tr>
        <td>Checklist</td>
        <td>40.92</td>
        <td>47.33</td>
        <td>64.92</td>
        <td>61.75</td>
        <td>73.25</td>
        <td>64.50</td>
        <td>70.25</td>
        <td>23.50</td>
        <td>39.92</td>
        <td>44.17</td>
        <td>31.58</td>
        <td>11.33</td>
        <td>2.25</td>
        <td>30.00</td>
        <td>42.08</td>
        <td>7.08</td>
        </tr>
        
        <tr>
        <td rowspan="3">Gemini-1.5-Pro</td>
        <td>Overall</td>
        <td>61.61</td>
        <td>63.50</td>
        <td><strong>72.59</strong></td>
        <td>68.00</td>
        <td>65.50</td>
        <td>47.50</td>
        <td>78.00</td>
        <td>54.00</td>
        <td>52.00</td>
        <td><strong>56.28</strong></td>
        <td><strong>54.04</strong></td>
        <td><strong>68.50</strong></td>
        <td>89.50</td>
        <td>39.50</td>
        <td><strong>52.00</strong></td>
        <td>63.27</td>
        </tr>
        <tr>
        <td>Rubric</td>
        <td>64.86</td>
        <td>66.33</td>
        <td>70.58</td>
        <td>74.25</td>
        <td>75.67</td>
        <td>72.17</td>
        <td>80.92</td>
        <td>71.42</td>
        <td>50.50</td>
        <td>48.83</td>
        <td>51.08</td>
        <td>65.75</td>
        <td>89.75</td>
        <td><strong>41.08</strong></td>
        <td>47.67</td>
        <td>66.83</td>
        </tr>
        <tr>
        <td>Checklist</td>
        <td><strong>67.91</strong></td>
        <td><strong>72.33</strong></td>
        <td>72.58</td>
        <td><strong>81.33</strong></td>
        <td><strong>75.92</strong></td>
        <td><strong>72.58</strong></td>
        <td><strong>94.42</strong></td>
        <td><strong>72.75</strong></td>
        <td><strong>55.33</strong></td>
        <td>52.92</td>
        <td>53.83</td>
        <td>65.75</td>
        <td><strong>90.25</strong></td>
        <td>40.42</td>
        <td>49.83</td>
        <td><strong>68.42</strong></td>
        </tr>
        
        <tr>
        <td rowspan="3">Gemini-2.0-Flash</td>
        <td>Overall</td>
        <td><strong>54.31</strong></td>
        <td><strong>42.00</strong></td>
        <td><strong>65.99</strong></td>
        <td><strong>57.00</strong></td>
        <td><strong>62.50</strong></td>
        <td>50.50</td>
        <td><strong>51.00</strong></td>
        <td>57.50</td>
        <td><strong>44.00</strong></td>
        <td><strong>55.78</strong></td>
        <td>42.93</td>
        <td>57.00</td>
        <td>77.50</td>
        <td><strong>47.50</strong></td>
        <td><strong>63.00</strong></td>
        <td><strong>40.31</strong></td>
        </tr>
        <tr>
        <td>Rubric</td>
        <td>42.20</td>
        <td>32.92</td>
        <td>51.50</td>
        <td>42.25</td>
        <td>50.08</td>
        <td>57.42</td>
        <td>46.58</td>
        <td>55.25</td>
        <td>36.25</td>
        <td>49.25</td>
        <td>36.42</td>
        <td>31.67</td>
        <td>56.92</td>
        <td>36.25</td>
        <td>30.58</td>
        <td>19.67</td>
        </tr>
        <tr>
        <td>Checklist</td>
        <td>51.16</td>
        <td>37.58</td>
        <td>59.25</td>
        <td>49.75</td>
        <td>55.17</td>
        <td><strong>67.58</strong></td>
        <td>30.67</td>
        <td><strong>62.67</strong></td>
        <td>37.42</td>
        <td>50.67</td>
        <td><strong>43.67</strong></td>
        <td><strong>60.25</strong></td>
        <td><strong>78.33</strong></td>
        <td>40.17</td>
        <td>57.75</td>
        <td>36.50</td>
        </tr>
        
        <!-- Score Section -->
        <tr><td colspan="18" class="has-text-centered has-background-grey-lighter"><strong>Score</strong></td></tr>
        <tr>
        <td rowspan="3">GPT-4o</td>
        <td>Overall</td>
        <td>35.53</td>
        <td>37.75</td>
        <td>44.00</td>
        <td>33.25</td>
        <td>38.50</td>
        <td>39.25</td>
        <td>40.25</td>
        <td><strong>43.50</strong></td>
        <td>43.25</td>
        <td>44.25</td>
        <td>26.75</td>
        <td>30.00</td>
        <td>13.25</td>
        <td>25.75</td>
        <td><strong>40.50</strong></td>
        <td><strong>32.75</strong></td>
        </tr>
        <tr>
        <td>Rubric</td>
        <td>35.83</td>
        <td>50.00</td>
        <td>48.79</td>
        <td>38.04</td>
        <td>42.96</td>
        <td>39.08</td>
        <td><strong>45.25</strong></td>
        <td>28.17</td>
        <td><strong>44.92</strong></td>
        <td>37.54</td>
        <td>30.79</td>
        <td><strong>32.00</strong></td>
        <td>12.50</td>
        <td><strong>28.08</strong></td>
        <td>33.46</td>
        <td>25.79</td>
        </tr>
        <tr>
        <td>Checklist</td>
        <td><strong>37.56</strong></td>
        <td><strong>52.33</strong></td>
        <td><strong>52.25</strong></td>
        <td><strong>38.88</strong></td>
        <td><strong>44.17</strong></td>
        <td><strong>46.54</strong></td>
        <td>43.29</td>
        <td>28.92</td>
        <td>44.83</td>
        <td><strong>48.42</strong></td>
        <td><strong>32.46</strong></td>
        <td>25.37</td>
        <td><strong>21.79</strong></td>
        <td>26.92</td>
        <td>31.75</td>
        <td>25.54</td>
        </tr>
        
        <tr>
        <td rowspan="3">Gemini-1.5-Pro</td>
        <td>Overall</td>
        <td>36.82</td>
        <td>39.75</td>
        <td>44.00</td>
        <td>39.50</td>
        <td>36.75</td>
        <td>28.00</td>
        <td>52.25</td>
        <td>14.25</td>
        <td>42.25</td>
        <td>52.25</td>
        <td>27.00</td>
        <td>35.00</td>
        <td>35.75</td>
        <td>28.25</td>
        <td><strong>48.50</strong></td>
        <td>28.75</td>
        </tr>
        <tr>
        <td>Rubric</td>
        <td>40.79</td>
        <td>48.54</td>
        <td>49.63</td>
        <td>38.96</td>
        <td>46.83</td>
        <td>39.79</td>
        <td>46.13</td>
        <td>36.54</td>
        <td>45.67</td>
        <td>57.63</td>
        <td>44.92</td>
        <td>35.46</td>
        <td>32.75</td>
        <td>26.29</td>
        <td>40.54</td>
        <td>22.21</td>
        </tr>
        <tr>
        <td>Checklist</td>
        <td><strong>53.16</strong></td>
        <td><strong>66.38</strong></td>
        <td><strong>64.71</strong></td>
        <td><strong>51.04</strong></td>
        <td><strong>61.12</strong></td>
        <td><strong>70.88</strong></td>
        <td><strong>57.96</strong></td>
        <td><strong>46.71</strong></td>
        <td><strong>55.25</strong></td>
        <td><strong>61.21</strong></td>
        <td><strong>68.63</strong></td>
        <td><strong>46.71</strong></td>
        <td><strong>41.29</strong></td>
        <td><strong>32.08</strong></td>
        <td>45.92</td>
        <td><strong>32.08</strong></td>
        </tr>
        
        <tr>
        <td rowspan="3">Gemini-2.0-Flash</td>
        <td>Overall</td>
        <td>36.13</td>
        <td>36.75</td>
        <td>35.00</td>
        <td>38.75</td>
        <td>34.75</td>
        <td>29.00</td>
        <td><strong>52.75</strong></td>
        <td>23.00</td>
        <td>37.50</td>
        <td>42.00</td>
        <td>34.50</td>
        <td><strong>52.50</strong></td>
        <td>11.50</td>
        <td><strong>39.75</strong></td>
        <td><strong>59.75</strong></td>
        <td>14.50</td>
        </tr>
        <tr>
        <td>Rubric</td>
        <td>34.00</td>
        <td>49.79</td>
        <td>47.13</td>
        <td>38.33</td>
        <td>40.58</td>
        <td>29.96</td>
        <td>47.00</td>
        <td>29.33</td>
        <td>38.13</td>
        <td>30.46</td>
        <td>35.54</td>
        <td>33.71</td>
        <td>12.21</td>
        <td>31.58</td>
        <td>33.83</td>
        <td>12.42</td>
        </tr>
        <tr>
        <td>Checklist</td>
        <td><strong>40.03</strong></td>
        <td><strong>50.33</strong></td>
        <td><strong>48.79</strong></td>
        <td><strong>41.13</strong></td>
        <td><strong>42.58</strong></td>
        <td><strong>38.46</strong></td>
        <td>48.17</td>
        <td><strong>33.00</strong></td>
        <td><strong>43.58</strong></td>
        <td><strong>52.75</strong></td>
        <td><strong>40.13</strong></td>
        <td>43.25</td>
        <td><strong>19.96</strong></td>
        <td>32.88</td>
        <td>48.54</td>
        <td><strong>17.04</strong></td>
        </tr>
        
        </tbody>
        </table>
        </div>
        </div>
        </div>
  </div>
</section>

<!-- OmniArena Section -->
<section class="section has-background-light">
  <div class="container is-max-desktop">
    <!-- 主标题 -->
    <div class="has-text-centered mb-6">
      <h2 class="title is-1 has-text-primary">OMNIARENA</h2>
      <p class="subtitle is-4">
        Universal Benchmark Platform for Omni-Model Evaluation
      </p>
    </div>

    <!-- 核心架构 -->
    <div class="columns is-vcentered mb-6">
      <div class="column is-half">
        <div class="content">
          <h3 class="title is-3">Core Architecture</h3>
          <div class="timeline">
            <div class="timeline-item">
              <div class="timeline-marker is-primary"></div>
              <div class="timeline-content">
                <p class="heading">Dynamic Evaluation</p>
                <p>Pairwise comparison mechanism with real-time ELO ranking</p>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker is-info"></div>
              <div class="timeline-content">
                <p class="heading">Open Participation</p>
                <p>Seamless integration for new models and custom queries</p>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker is-success"></div>
              <div class="timeline-content">
                <p class="heading">Dual Arena System</p>
                <p>Specialized evaluation tracks for MMU and MMG tasks</p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="column is-half">
        <div class="box has-background-white">
          <img src="static/images/ArenaResult.jpg" alt="Platform Raw Result" class="has-ratio" style="width:100%">
        </div>
      </div>
    </div>



  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/paper/judgeanything.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Source code borrows from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
